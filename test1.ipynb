{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> \n",
      "\n",
      "41 \n",
      "\n",
      "<class 'llama_index.core.schema.Document'>\n",
      "Doc ID: ac2dc905-fc5f-4fd8-ba49-008f3be81b90\n",
      "Text: PAGE 1 Founder, DeepLearning.AI Collected Insights from Andrew\n",
      "Ng How to  Build Your Career in AI A Simple Guide\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Loading data (Ingestion)\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\"./eBook-How-to-Build-a-Career-in-AI.pdf\"]\n",
    ").load_data()\n",
    "\n",
    "print(type(documents), \"\\n\")\n",
    "print(len(documents), \"\\n\")\n",
    "print(type(documents[0]))\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'llama_index.core.schema.Document'>\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Document\n",
    "\n",
    "document = Document(text=\"\\n\\n\".join([doc.text for doc in documents]))\n",
    "\n",
    "print(type(document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of course, I also encourage learning driven by curiosity. If something interests you, go ahead \n",
      "and learn it regardless of how useful it might turn out to be!  Maybe this will lead to a creative \n",
      "spark or technical breakthrough.\n",
      "How much math do you need to know to be a machine learning engineer?\n"
     ]
    }
   ],
   "source": [
    "# Defining the hierarchical node parser\n",
    "# Splits a document into a recursive hierarchy Nodes\n",
    "from llama_index.core.node_parser import HierarchicalNodeParser\n",
    "\n",
    "# top-level nodes - chunk size 2048\n",
    "# second-level nodes - chunk size 512\n",
    "# third-level nodes - chunk size 128\n",
    "# each parent node contains 4 children nodes\n",
    "node_parser = HierarchicalNodeParser.from_defaults(\n",
    "    chunk_sizes=[2048, 512, 128]\n",
    ")\n",
    "\n",
    "# Parsing the document into a hierarchy of nodes\n",
    "nodes = node_parser.get_nodes_from_documents([document])\n",
    "\n",
    "\n",
    "from llama_index.core.node_parser import get_leaf_nodes\n",
    "\n",
    "leaf_nodes = get_leaf_nodes(nodes)\n",
    "print(leaf_nodes[30].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting LLM and ebemdding model\n",
    "# from llama_index.llms.openai import OpenAI\n",
    "# from llama_index.core import Settings\n",
    "\n",
    "# Settings.llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)\n",
    "\n",
    "# from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "# Settings.embed_model = OpenAIEmbedding(\n",
    "#     model=\"text-embedding-3-small\", embed_batch_size=100\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:pinecone_plugin_interface.logging:Discovering subpackages in _NamespacePath(['/Users/panda/Desktop/RAG-Based-Research-Paper-Assistant/venv/lib/python3.11/site-packages/pinecone_plugins'])\n",
      "Discovering subpackages in _NamespacePath(['/Users/panda/Desktop/RAG-Based-Research-Paper-Assistant/venv/lib/python3.11/site-packages/pinecone_plugins'])\n",
      "INFO:pinecone_plugin_interface.logging:Looking for plugins in pinecone_plugins.inference\n",
      "Looking for plugins in pinecone_plugins.inference\n",
      "INFO:pinecone_plugin_interface.logging:Installing plugin inference into Pinecone\n",
      "Installing plugin inference into Pinecone\n",
      "INFO:pinecone_plugin_interface.logging:Discovering subpackages in _NamespacePath(['/Users/panda/Desktop/RAG-Based-Research-Paper-Assistant/venv/lib/python3.11/site-packages/pinecone_plugins'])\n",
      "Discovering subpackages in _NamespacePath(['/Users/panda/Desktop/RAG-Based-Research-Paper-Assistant/venv/lib/python3.11/site-packages/pinecone_plugins'])\n",
      "INFO:pinecone_plugin_interface.logging:Looking for plugins in pinecone_plugins.inference\n",
      "Looking for plugins in pinecone_plugins.inference\n",
      "{'dimension': 1536,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {},\n",
      " 'total_vector_count': 0}\n",
      "Indexing documents to Pinecone...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07a4263aee5847e9b57046e103c9220e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upserted vectors:   0%|          | 0/110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.vector_stores.pinecone import PineconeVectorStore\n",
    "from IPython.display import Markdown, display\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "\n",
    "import time\n",
    "\n",
    "index_name = \"rag-based-research-paper-assistant\"\n",
    "# exists = pc.Index(index_name).exists()\n",
    "existing_indexes = [\n",
    "    index_info[\"name\"] for index_info in pc.list_indexes()\n",
    "]\n",
    "\n",
    "# check if index already exists (it shouldn't if this is first time)\n",
    "if index_name not in existing_indexes:\n",
    "    # if does not exist, create index\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=1536,\n",
    "        spec=ServerlessSpec(\n",
    "            cloud=\"aws\",\n",
    "            region=\"us-east-1\"\n",
    "        )\n",
    "    )\n",
    "    # wait for index to be initialized\n",
    "    while not pc.describe_index(index_name).status['ready']:\n",
    "        time.sleep(1)\n",
    "\n",
    "# connect to index\n",
    "pinecone_index = pc.Index(index_name)\n",
    "time.sleep(1)\n",
    "# view index stats\n",
    "print(pinecone_index.describe_index_stats())\n",
    "index_stats = pinecone_index.describe_index_stats()\n",
    "\n",
    "# initialize vector store\n",
    "vector_store = PineconeVectorStore(\n",
    "    pinecone_index=pinecone_index,\n",
    "    text_key=\"text\"\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# Only index documents if the index is empty\n",
    "if index_stats.total_vector_count == 0:\n",
    "    print(\"Indexing documents to Pinecone...\")\n",
    "    storage_context.docstore.add_documents(nodes)\n",
    "    automerging_index = VectorStoreIndex(\n",
    "        leaf_nodes,\n",
    "        storage_context=storage_context,\n",
    "    )\n",
    "else:\n",
    "    print(f\"Using existing index with {index_stats.total_vector_count} vectors\")\n",
    "    automerging_index = VectorStoreIndex.from_vector_store(vector_store)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Storage and Indexing\n",
    "# import os\n",
    "# from llama_index.core import VectorStoreIndex, StorageContext\n",
    "# from llama_index.core import load_index_from_storage\n",
    "\n",
    "# # check if the index already exists\n",
    "# # if not, create a new index\n",
    "# if not os.path.exists(\"./merging_index_test1\"):\n",
    "#     # Creating a storage context\n",
    "#     # A utility container for storing nodes, indices, and vectors\n",
    "#     storage_context = StorageContext.from_defaults()\n",
    "#     storage_context.docstore.add_documents(nodes)\n",
    "\n",
    "#     # Creating a vector store index\n",
    "#     automerging_index = VectorStoreIndex(\n",
    "#             leaf_nodes,\n",
    "#             storage_context=storage_context,\n",
    "#             # service_context=auto_merging_context\n",
    "#         )\n",
    "\n",
    "#     # Persisting the index\n",
    "#     automerging_index.storage_context.persist(persist_dir=\"./merging_index_test1\")\n",
    "# else:\n",
    "#     # Loading the index from storage\n",
    "#     automerging_index = load_index_from_storage(\n",
    "#         StorageContext.from_defaults(persist_dir=\"./merging_index_test1\"),\n",
    "#         # service_context=auto_merging_context\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the Query Engine\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.core.retrievers import AutoMergingRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "# Creating a retriever, get top 12 similar nodes\n",
    "automerging_retriever = automerging_index.as_retriever(\n",
    "    similarity_top_k=12\n",
    ")\n",
    "\n",
    "# When combined with the HierarchicalNodeParser, this enables us to\n",
    "    # automatically replace retrieved nodes with their parents \n",
    "        # when a majority of children are retrieved.\n",
    "retriever = AutoMergingRetriever(\n",
    "    automerging_retriever, \n",
    "    automerging_index.storage_context, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Reranking the retrieved nodes and returning the top 6\n",
    "rerank = SentenceTransformerRerank(top_n=6, model=\"BAAI/bge-reranker-base\")\n",
    "\n",
    "# Creating the query engine\n",
    "auto_merging_engine = RetrieverQueryEngine.from_args(\n",
    "    # automerging_retriever, node_postprocessors=[rerank]\n",
    "    retriever, node_postprocessors=[rerank]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:llama_index.core.retrievers.auto_merging_retriever:> Merging 3 nodes into parent node.\n",
      "> Parent node id: d704f99c-fcfd-4a69-8ed7-265566a31128.\n",
      "> Parent node text: LEARNING\n",
      "PROJECTS\n",
      "JOB\n",
      "\n",
      "PAGE 8\n",
      "Learning Technical \n",
      "Skills for a Promising \n",
      "AI Career\n",
      "CHAPTER 2\n",
      "LEA...\n",
      "\n",
      "> Merging 3 nodes into parent node.\n",
      "> Parent node id: d704f99c-fcfd-4a69-8ed7-265566a31128.\n",
      "> Parent node text: LEARNING\n",
      "PROJECTS\n",
      "JOB\n",
      "\n",
      "PAGE 8\n",
      "Learning Technical \n",
      "Skills for a Promising \n",
      "AI Career\n",
      "CHAPTER 2\n",
      "LEA...\n",
      "\n",
      "> Merging 3 nodes into parent node.\n",
      "> Parent node id: d704f99c-fcfd-4a69-8ed7-265566a31128.\n",
      "> Parent node text: LEARNING\n",
      "PROJECTS\n",
      "JOB\n",
      "\n",
      "PAGE 8\n",
      "Learning Technical \n",
      "Skills for a Promising \n",
      "AI Career\n",
      "CHAPTER 2\n",
      "LEA...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32a4d5c462374baa8126f4e23233234f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Start by learning foundational technical skills such as machine learning models, deep learning, software development, and math relevant to machine learning. Prioritize topic selection and understand core concepts behind machine learning. Additionally, work on projects to gain practical experience and build a portfolio that shows skill progression. Finally, find the right AI job for you by using informational interviews, scoping successful AI projects, and aligning projects with your career goals.\n"
     ]
    }
   ],
   "source": [
    "auto_merging_response = auto_merging_engine.query(\n",
    "    # 'How do I pick projects to strengthen my AI resume?'\n",
    "    # \"What is the importance of networking in AI?\"\n",
    "    'What are the best practices for building a career in AI?'\n",
    ")\n",
    "print(auto_merging_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
