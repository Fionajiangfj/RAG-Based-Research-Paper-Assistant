{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Ignoring wrong pointing object 22 0 (offset 0)\n",
      "Ignoring wrong pointing object 28 0 (offset 0)\n",
      "Ignoring wrong pointing object 30 0 (offset 0)\n",
      "Ignoring wrong pointing object 36 0 (offset 0)\n",
      "Ignoring wrong pointing object 38 0 (offset 0)\n",
      "Ignoring wrong pointing object 40 0 (offset 0)\n",
      "Ignoring wrong pointing object 45 0 (offset 0)\n",
      "Ignoring wrong pointing object 55 0 (offset 0)\n",
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Ignoring wrong pointing object 38 0 (offset 0)\n",
      "Ignoring wrong pointing object 72 0 (offset 0)\n",
      "Ignoring wrong pointing object 74 0 (offset 0)\n",
      "Ignoring wrong pointing object 76 0 (offset 0)\n",
      "Ignoring wrong pointing object 116 0 (offset 0)\n",
      "Ignoring wrong pointing object 127 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2014 documents\n",
      "\n",
      "Sample document:\n",
      "Document ID: /Users/panda/Desktop/RAG-Based-Research-Paper-Assistant/papers/2502.01651v1.pdf_part_0\n",
      "Text preview: همایش بینالمللی هوش مصنوعی و تمدن آینده  \n",
      "International conference on Artificial Intelligence and \n",
      "Future Civilization \n",
      "Icai.ihu.ac.ir \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "1 \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Fine Tunning LLaMA 2 Inte...\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Loading data (Ingestion)\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "import os\n",
    "\n",
    "# Create papers directory if it doesn't exist\n",
    "papers_dir = \"./papers\"\n",
    "if not os.path.exists(papers_dir):\n",
    "    os.makedirs(papers_dir)\n",
    "\n",
    "# Load all PDF files from the papers directory and its subdirectories\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_dir=papers_dir,\n",
    "    recursive=True,  # Include subdirectories\n",
    "    filename_as_id=True,  # Use filenames as document IDs\n",
    "    required_exts=['.pdf']  # Only load PDF files\n",
    ").load_data()\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents\")\n",
    "print(\"\\nSample document:\")\n",
    "print(f\"Document ID: {documents[0].doc_id}\")\n",
    "print(f\"Text preview: {documents[0].text[:200]}...\")\n",
    "# documents = SimpleDirectoryReader(\n",
    "#     input_files=[\"./eBook-How-to-Build-a-Career-in-AI.pdf\"]\n",
    "# ).load_data()\n",
    "\n",
    "# print(type(documents), \"\\n\")\n",
    "# print(len(documents), \"\\n\")\n",
    "# print(type(documents[0]))\n",
    "# print(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'llama_index.core.schema.Document'>\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Document\n",
    "\n",
    "document = Document(text=\"\\n\\n\".join([doc.text for doc in documents]))\n",
    "\n",
    "print(type(document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "• C++ and Zig implementations exhibit strong performance, particularly in larger models. This underscores their \n",
      "suitability for computationally intensive tasks and effective utilization of modern hardware capabilities. \n",
      "• Go, Rust, and Julia perform moderately, offering reasonable efficiency for LLM inference tasks. Additional \n",
      "optimizations and platform-specific tuning could further enhance their performance \n",
      " \n",
      "        \n",
      " \n",
      "(a): Average tokens per second for stories15M.bin model    (b): Average time per inference for stories15M.\n"
     ]
    }
   ],
   "source": [
    "# Defining the hierarchical node parser\n",
    "# Splits a document into a recursive hierarchy Nodes\n",
    "from llama_index.core.node_parser import HierarchicalNodeParser\n",
    "\n",
    "# top-level nodes - chunk size 2048\n",
    "# second-level nodes - chunk size 512\n",
    "# third-level nodes - chunk size 128\n",
    "# each parent node contains 4 children nodes\n",
    "node_parser = HierarchicalNodeParser.from_defaults(\n",
    "    chunk_sizes=[2048, 512, 128]\n",
    ")\n",
    "\n",
    "# Parsing the document into a hierarchy of nodes\n",
    "nodes = node_parser.get_nodes_from_documents([document])\n",
    "\n",
    "\n",
    "from llama_index.core.node_parser import get_leaf_nodes\n",
    "\n",
    "leaf_nodes = get_leaf_nodes(nodes)\n",
    "print(leaf_nodes[30].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting LLM and ebemdding model\n",
    "# from llama_index.llms.openai import OpenAI\n",
    "# from llama_index.core import Settings\n",
    "\n",
    "# Settings.llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)\n",
    "\n",
    "# from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "# Settings.embed_model = OpenAIEmbedding(\n",
    "#     model=\"text-embedding-3-small\", embed_batch_size=100\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dimension': 1536,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {},\n",
      " 'total_vector_count': 0}\n",
      "Indexing documents to Pinecone...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe3018c4e8c344d79e6a960e35928838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upserted vectors:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf0f6dc433504de9b52b9db5dd7dec9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upserted vectors:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea5b02333a83487989d8a52035f62977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upserted vectors:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd62aab5baad40a0a3dec2348d99c859",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upserted vectors:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c195240b9d74476ba45b22b4e1c7b6bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upserted vectors:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ee1d1a6555e4c57b7a8856e57fd686e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upserted vectors:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "431489d727024e778db235488585816c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upserted vectors:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fce28d7564714616aa5af9ea1eee1220",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upserted vectors:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fb4853f1de744d0bf1fda808bb83b47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upserted vectors:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "914e0230007346aca397476be5cf5096",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upserted vectors:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8b33dd0be45486a8c833814844077a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upserted vectors:   0%|          | 0/1186 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.vector_stores.pinecone import PineconeVectorStore\n",
    "from IPython.display import Markdown, display\n",
    "from llama_index.core import StorageContext\n",
    "import time\n",
    "\n",
    "load_dotenv()\n",
    "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "index_name = \"rag-based-research-paper-assistant-v2\"\n",
    "\n",
    "existing_indexes = [\n",
    "    index_info[\"name\"] for index_info in pc.list_indexes()\n",
    "]\n",
    "\n",
    "# check if index already exists (it shouldn't if this is first time)\n",
    "if index_name not in existing_indexes:\n",
    "    # if does not exist, create index\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=1536,\n",
    "        spec=ServerlessSpec(\n",
    "            cloud=\"aws\",\n",
    "            region=\"us-east-1\"\n",
    "        )\n",
    "    )\n",
    "    # wait for index to be initialized\n",
    "    while not pc.describe_index(index_name).status['ready']:\n",
    "        time.sleep(1)\n",
    "\n",
    "# connect to index\n",
    "pinecone_index = pc.Index(index_name)\n",
    "time.sleep(1)\n",
    "# view index stats\n",
    "print(pinecone_index.describe_index_stats())\n",
    "\n",
    "# initialize vector store\n",
    "vector_store = PineconeVectorStore(\n",
    "    pinecone_index=pinecone_index,\n",
    "    text_key=\"text\"\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# Get current index stats\n",
    "index_stats = pinecone_index.describe_index_stats()\n",
    "\n",
    "# Only index documents if the index is empty\n",
    "if index_stats.total_vector_count == 0:\n",
    "    print(\"Indexing documents to Pinecone...\")\n",
    "    storage_context.docstore.add_documents(nodes)\n",
    "    automerging_index = VectorStoreIndex(\n",
    "        leaf_nodes,\n",
    "        storage_context=storage_context,\n",
    "    )\n",
    "else:\n",
    "    print(f\"Using existing index with {index_stats.total_vector_count} vectors\")\n",
    "    automerging_index = VectorStoreIndex.from_vector_store(vector_store)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Storage and Indexing\n",
    "# import os\n",
    "# from llama_index.core import VectorStoreIndex, StorageContext\n",
    "# from llama_index.core import load_index_from_storage\n",
    "\n",
    "# # check if the index already exists\n",
    "# # if not, create a new index\n",
    "# if not os.path.exists(\"./merging_index_test1\"):\n",
    "#     # Creating a storage context\n",
    "#     # A utility container for storing nodes, indices, and vectors\n",
    "#     storage_context = StorageContext.from_defaults()\n",
    "#     storage_context.docstore.add_documents(nodes)\n",
    "\n",
    "#     # Creating a vector store index\n",
    "#     automerging_index = VectorStoreIndex(\n",
    "#             leaf_nodes,\n",
    "#             storage_context=storage_context,\n",
    "#             # service_context=auto_merging_context\n",
    "#         )\n",
    "\n",
    "#     # Persisting the index\n",
    "#     automerging_index.storage_context.persist(persist_dir=\"./merging_index_test1\")\n",
    "# else:\n",
    "#     # Loading the index from storage\n",
    "#     automerging_index = load_index_from_storage(\n",
    "#         StorageContext.from_defaults(persist_dir=\"./merging_index_test1\"),\n",
    "#         # service_context=auto_merging_context\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the Query Engine\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.core.retrievers import AutoMergingRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "# Creating a retriever, get top 12 similar nodes\n",
    "automerging_retriever = automerging_index.as_retriever(\n",
    "    similarity_top_k=12\n",
    ")\n",
    "\n",
    "# When combined with the HierarchicalNodeParser, this enables us to\n",
    "    # automatically replace retrieved nodes with their parents \n",
    "        # when a majority of children are retrieved.\n",
    "retriever = AutoMergingRetriever(\n",
    "    automerging_retriever, \n",
    "    automerging_index.storage_context, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Reranking the retrieved nodes and returning the top 6\n",
    "rerank = SentenceTransformerRerank(top_n=6, model=\"BAAI/bge-reranker-base\")\n",
    "\n",
    "# Creating the query engine\n",
    "auto_merging_engine = RetrieverQueryEngine.from_args(\n",
    "    # automerging_retriever, node_postprocessors=[rerank]\n",
    "    retriever, node_postprocessors=[rerank]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Merging 3 nodes into parent node.\n",
      "> Parent node id: 98558be2-0d0b-4ca5-b7ae-585d3c050d8f.\n",
      "> Parent node text: This suggests that\n",
      "BioGPT leverages its internal knowledge more effectively but does\n",
      "not fully ex...\n",
      "\n",
      "KEDRec-LM is a novel framework designed to advance drug discovery by integrating structured biomedical knowledge and unstructured textual data. It is an instruction-tuned Large Language Model (LLM) that distills knowledge from a rich medical knowledge corpus for drug recommendation and rationale generation.\n"
     ]
    }
   ],
   "source": [
    "auto_merging_response = auto_merging_engine.query(\n",
    "    'What is \"KEDRec-LM\"?'\n",
    ")\n",
    "print(auto_merging_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
